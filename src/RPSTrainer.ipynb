{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock-Paper-Scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Worked Example: Rock-Paper-Scissors\n",
    "The code below is a faithful Python re-implementation of the original Java version `RPSTrainer.java` as developed in section 2.4 of Neller's & Lanctot's lecture notes. This code can be run directly from `RPSTrainer0.py` in this repository.\n",
    "\n",
    "Note that the Python version has less code than the Java version, thanks to builtin ranges and `NumPy`'s vectorized operations such as `+=`, `/=` and `/` and builtin functions such as `sum`, `clip`, `repeat` and `zeros`. We have also leveraged `NumPy`'s `random.choice` to sample actions from a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.32310306e-04 9.99767356e-01 3.33333333e-07]\n"
     ]
    }
   ],
   "source": [
    "# %load RPSTrainer0.py\n",
    "#          Copyright Rein Halbersma 2018.\n",
    "# Distributed under the Boost Software License, Version 1.0.\n",
    "#    (See accompanying file LICENSE_1_0.txt or copy at\n",
    "#          http://www.boost.org/LICENSE_1_0.txt)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RPSTrainer:\n",
    "    def __init__(self):\n",
    "        self.NUM_ACTIONS = 3\n",
    "        self.regretSum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategySum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.oppStrategy = [.4, .3, .3]\n",
    "    \n",
    "    def getStrategy(self):\n",
    "        strategy = self.regretSum.clip(min=0)\n",
    "        normalizingSum = np.sum(strategy)\n",
    "        if normalizingSum > 0:\n",
    "            strategy /= normalizingSum\n",
    "        else:\n",
    "            strategy = np.repeat(1 / self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        self.strategySum += strategy\n",
    "        return strategy\n",
    "    \n",
    "    def getAction(self, strategy):\n",
    "        return np.random.choice(len(strategy), p=strategy)\n",
    "    \n",
    "    def train(self, iterations):\n",
    "        actionUtility = np.zeros(self.NUM_ACTIONS)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            strategy = self.getStrategy()\n",
    "            myAction = self.getAction(strategy)\n",
    "            otherAction = self.getAction(self.oppStrategy)\n",
    "            \n",
    "            actionUtility[otherAction] = 0\n",
    "            actionUtility[0 if otherAction == self.NUM_ACTIONS - 1 else otherAction + 1] = 1\n",
    "            actionUtility[self.NUM_ACTIONS - 1 if otherAction == 0 else otherAction - 1] = -1\n",
    "            \n",
    "            for a in range(self.NUM_ACTIONS):\n",
    "                self.regretSum[a] += actionUtility[a] - actionUtility[myAction]\n",
    "    \n",
    "    def getAverageStrategy(self):\n",
    "        normalizingSum = np.sum(self.strategySum)\n",
    "        if normalizingSum > 0:\n",
    "            avgStrategy = self.strategySum / normalizingSum\n",
    "        else:\n",
    "            avgStrategy = np.repeat(1 / self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return avgStrategy\n",
    "        \n",
    "def main():\n",
    "    trainer = RPSTrainer()\n",
    "    trainer.train(1000000)\n",
    "    print(trainer.getAverageStrategy())\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code shows that after training for 1 million iterations against an opponent playing Rock 40% of the time, the average strategy has converged to the best response of playing Paper 100% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the rate of convergence\n",
    "\n",
    "In order to know how fast the algorithm converges, we adapt the code to save the intermediate average strategies and its exploitability. We then make a log-log plots of these quantities versus the iteration number. This code can be run directly from `RPSTrainer1.py` in this repository.\n",
    "\n",
    "Note that we refactored the code a bit compared to version 0: moving the update of `strategySum` from `getStrategy()` to `train()`, factoring the normalization of a strategy vector into a separate `normalize()` function, adding `bestResponse()` and `exploitability()` functions and moving the `actionUtility` into a matrix. Finally, the `train()` function now returns a `Pandas` data frame that can be analyzed after training has completed. In the spirit of Reinforcement Learning, the trained `averageStrategy()` is called `target_policy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load RPSTrainer1.py\n",
    "#          Copyright Rein Halbersma 2018.\n",
    "# Distributed under the Boost Software License, Version 1.0.\n",
    "#    (See accompanying file LICENSE_1_0.txt or copy at\n",
    "#          http://www.boost.org/LICENSE_1_0.txt)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "class RPSTrainer:\n",
    "    def __init__(self):\n",
    "        self.NUM_ACTIONS = 3\n",
    "        self.actionUtility = np.array([\n",
    "            [0, -1, 1],\n",
    "            [1, 0, -1],\n",
    "            [-1, 1, 0]\n",
    "        ])        \n",
    "        self.regretSum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.strategySum = np.zeros(self.NUM_ACTIONS)\n",
    "        self.oppStrategy = [.4, .3, .3]\n",
    "    \n",
    "    def normalize(self, strategy):\n",
    "        normalizingSum = np.sum(strategy)\n",
    "        if normalizingSum > 0:\n",
    "            strategy /= normalizingSum\n",
    "        else:\n",
    "            strategy = np.repeat(1 / self.NUM_ACTIONS, self.NUM_ACTIONS)\n",
    "        return strategy       \n",
    "    \n",
    "    def getStrategy(self):\n",
    "        return self.normalize(self.regretSum.clip(min=0))\n",
    "    \n",
    "    def getAverageStrategy(self):\n",
    "        return self.normalize(np.copy(self.strategySum))\n",
    "        \n",
    "    def getAction(self, strategy):\n",
    "        return np.random.choice(len(strategy), p=strategy)\n",
    "    \n",
    "    def bestResponse(self, utility): \n",
    "        return np.eye(self.NUM_ACTIONS)[np.argmax(utility)]\n",
    "        \n",
    "    def exploitability(self, strategy):\n",
    "        utility = np.dot(self.actionUtility, self.oppStrategy)\n",
    "        return np.dot(self.bestResponse(utility) - strategy, utility)\n",
    "    \n",
    "    def train(self, iterations, df=None, sample=.001):        \n",
    "        for i in range(iterations):\n",
    "            strategy = self.getStrategy()\n",
    "            self.strategySum += strategy\n",
    "            myAction = self.getAction(strategy)\n",
    "            otherAction = self.getAction(self.oppStrategy)\n",
    "            \n",
    "            self.regretSum += self.actionUtility[:, otherAction] - self.actionUtility[myAction, otherAction]\n",
    "                \n",
    "            if df is None or np.random.random() > sample:\n",
    "                continue\n",
    "            target_policy = self.getAverageStrategy()\n",
    "            df = df.append(\n",
    "                pd.DataFrame(\n",
    "                    np.append(\n",
    "                        np.array([i, self.exploitability(target_policy)]),\n",
    "                        target_policy                        \n",
    "                    ).reshape(-1, 2 + self.NUM_ACTIONS), \n",
    "                    columns=list(df)\n",
    "                ), \n",
    "                ignore_index=True\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "def main():\n",
    "    columns = ['iterations', 'exploitability', 'rock', 'paper', 'scissors']\n",
    "    df = pd.DataFrame(columns=columns)        \n",
    "    \n",
    "    trainer = RPSTrainer()\n",
    "    df = trainer.train(1000000, df)\n",
    "    target_policy = trainer.getAverageStrategy()\n",
    "    \n",
    "    print('Target policy: %s' % (target_policy))\n",
    "    for c in columns[2:]:\n",
    "        plt.loglog(df[columns[0]], df[c], label=c)\n",
    "    plt.xlabel(columns[0])\n",
    "    plt.ylabel('target policy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print('Exploitability: %s' % (trainer.exploitability(target_policy)))\n",
    "    plt.loglog(df[columns[0]], df[columns[1]])\n",
    "    plt.xlabel(columns[0])\n",
    "    plt.ylabel(columns[1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    model = sm.ols(formula=\"np.log(exploitability) ~ np.log(iterations)\", data=df).fit()\n",
    "    print(model.params)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the convergence is inversely linear: to get exploitability less than $\\varepsilon$, we need at $\\mathcal{O}(\\varepsilon^{-1})$ iterations. A million iterations are needed to reduce exploitability to one part per million."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
